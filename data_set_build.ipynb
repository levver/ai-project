{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WE use yahoo_fin and yfinance as modules to read stock information.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yahoo_fin.stock_info as si\n",
    "import yfinance as yf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FIRST READ ALL STOCK TICKERS (OR MOST OF THEM, THE MOST RELEVANT)\n",
    "# we use the yahoo_fin.stock_info module in order to receive lists of ticker symbols.\n",
    "# we get approx. 8900 symbols (can change any day as it is updated constantly..)\n",
    "ticker1 = si.tickers_other()\n",
    "ticker2 = si.tickers_sp500()\n",
    "ticker3 = si.tickers_dow()\n",
    "ticker4 = si.tickers_nasdaq()\n",
    "infirst = set(ticker1)\n",
    "insecond = set(ticker2)\n",
    "result2 = ticker1 + list(insecond - infirst)\n",
    "infirst = set(result2)\n",
    "insecond = set(ticker3)\n",
    "result3 = result2 + list(insecond - infirst)\n",
    "infirst = set(result3)\n",
    "insecond = set(ticker4)\n",
    "all_tickers = result3 + list(insecond - infirst)\n",
    "all_tickers  # Now we have a list of stock symbols, with which we can use \n",
    "# the yfinance module to read info about them. (approx. 8900 symbols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# READ DATA FROM yfinance module. In both cases of training and test we tried to find\n",
    "# a period of time in which the economy is more or less stable, and the stock market\n",
    "# is more or less healthy. This way we can ensure that we try to learn the stock BEHAVIOR\n",
    "# without the effects of global issues.\n",
    "# For LEARNING, we used the period of 01.01.2011 to 31.12.2019\n",
    "# For TEST, we used period of 01.04.2003 to 31.05.2007. \n",
    "data_from_yfinance = yf.download(all_tickers, start='2011-01-01', end='2019-12-31')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save read data, used internaly for us. Used different name for test\n",
    "data_from_yfinance.to_pickle('raw_data_save.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read saved data, used inernaly for us. Used different name for test\n",
    "data_from_yfinance = pd.read_pickle('raw_data_save.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Begin cleaning the data, extracting the information that we need.  \n",
    "# extract only close and volume. From close we will derive 4 more indicators, so we will\n",
    "# have 6 indicators in total for each stock.\n",
    "\n",
    "close = data_from_yfinance['Close']\n",
    "volume = data_from_yfinance['Volume']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean the data. remove anything that isnt with full info of the given period.\n",
    "# each year has some days that the stock market is completely closed. In those days we have NaN values\n",
    "# so we remove them.\n",
    "close_clean = close.dropna(axis=0, how='all')  \n",
    "# Any stock that doesnt have full information of the given period will also be dropped.\n",
    "close_clean = close_clean.dropna(axis=1, how='any')  \n",
    "# Remove the date indication. Now each column is a stock, and rows are consecutive days\n",
    "close_clean = close_clean.reset_index(drop=True)\n",
    "# save the close price data.\n",
    "close_clean.to_csv('C:\\\\Users\\\\Daniel\\\\PycharmProjects\\\\StockData\\\\close_clean.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# same here, for volume. \n",
    "# we note that the columns and rows of close and volume correspond completely. \n",
    "# where there are values in close there are also values in volume, and vice versa.\n",
    "volume_clean = volume.dropna(axis=0, how='all')\n",
    "volume_clean = volume_clean.dropna(axis=1, how='any')\n",
    "volume_clean = volume_clean.reset_index(drop=True)\n",
    "volume_clean.to_csv('C:\\\\Users\\\\Daniel\\\\PycharmProjects\\\\StockData\\\\volume_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build the indicators. We decided to use the MA50 - 50 day moving average, MA200 - 200 day moving average,\n",
    "# EMA200 - 200 day exponential moving average, RSI - relative strength index. In total, with the close\n",
    "# price indicator and volume indicator we will have 6 valid indicators for all the stocks, for the \n",
    "# given period of time.\n",
    "# Here we calculate MA50, and save it for use internaly.\n",
    "ma50 = close_clean.rolling(50).mean()\n",
    "ma50.to_csv('C:\\\\Users\\\\Daniel\\\\PycharmProjects\\\\StockData\\\\ma50_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we calculate MA200\n",
    "ma200 = close_clean.rolling(200).mean()\n",
    "ma200.to_csv('C:\\\\Users\\\\Daniel\\\\PycharmProjects\\\\StockData\\\\ma200_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we calculate the EMA200\n",
    "ema200 = close_clean.ewm(span=200).mean()\n",
    "ema200.to_csv('C:\\\\Users\\\\Daniel\\\\PycharmProjects\\\\StockData\\\\ema200_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Here we calculate the RSI\n",
    "delta = close_clean.diff()\n",
    "up, down = delta.copy(), delta.copy()\n",
    "up[up < 0] = 0\n",
    "down[down > 0] = 0\n",
    "roll_up = up.rolling(14).mean()\n",
    "roll_down = down.abs().rolling(14).mean()\n",
    "RSI = 100.0 - (100.0/(1.0 + (roll_up/roll_down)))\n",
    "RSI.to_csv('C:\\\\Users\\\\Daniel\\\\PycharmProjects\\\\StockData\\\\RSI_test.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read currently available indicator, for internal use for us.\n",
    "close_clean = pd.read_csv('close_clean.csv')\n",
    "ma50 = pd.read_csv('ma50.csv')\n",
    "RSI = pd.read_csv('RSI.csv')\n",
    "ema200 = pd.read_csv('ema200.csv')\n",
    "ma200 = pd.read_csv('ma200.csv')\n",
    "volume = pd.read_csv('volume.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start the trimming process to create the actual database\n",
    "# because we want to use the 200 day moving average, the first 200 days of all the data is irrelevant \n",
    "# because it wont have the 200MA indicator\n",
    "# find the range of indexes relevant\n",
    "first_index = ma200.first_valid_index()\n",
    "last_index = ma200.last_valid_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove 200 first days of all data so that it corresponds to no data of MA200\n",
    "\n",
    "close_trimmed = close_clean.iloc[first_index:last_index, :].reset_index(drop=True)\n",
    "ma50_trimmed = ma50.iloc[first_index:last_index, :].reset_index(drop=True)\n",
    "ma200_trimmed = ma200.iloc[first_index:last_index, :].reset_index(drop=True)\n",
    "ema200_trimmed = ema200.iloc[first_index:last_index, :].reset_index(drop=True)\n",
    "RSI_trimmed = RSI.iloc[first_index:last_index, :].reset_index(drop=True)\n",
    "volume_trimmed = volume_clean.iloc[first_index:last_index, :].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Divide the trimmed data into 9 approximate years. generate random offsets for each stock\n",
    "# We want to use a 'more-or-less' year's worth of information about the stock in order to learn about it\n",
    "# and derive whether the next year will be at least 10% profitable from the last day of the \n",
    "# current year. \n",
    "# We had approx. 9 years of information, and removed the first 200 days. After some calculations, we decided\n",
    "# that a year will be exactly 229 days (which is close enough to an actual year - approx. 256 days of trading,\n",
    "# excluding saturday and sunday, and some non-trading days).\n",
    "# These calculations were performed early on in the work so we decided to stick with them.\n",
    "# Now, if we had divided the information as is into 229 day periods, there could be a lot of \n",
    "# correlation between stock of the same period, if for example those 229 days were strong days in the \n",
    "# stock market. So, in order to remove the correlation, we decided to offset each stock information\n",
    "# by some random value between 0 to 229. Here, we generate the offset value for each stock\n",
    "year_day_size = 229  # should be 229, or total/9\n",
    "start_indexes_for_stocks = np.random.randint(low=0, high=year_day_size, size=len(close.columns))\n",
    "start_indexes_for_stocks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We shift each stock to stock[offset:-(year-offset), so that we are left with one less year of information\n",
    "# for each stock, and the stock are uncorrelated with regards to date.\n",
    "# after this, each stock contains 8 clean years(229) of info\n",
    "offseted_close = pd.DataFrame(columns=close_trimmed.columns)\n",
    "offseted_ma50 = pd.DataFrame(columns=close_trimmed.columns)\n",
    "offseted_ma200 = pd.DataFrame(columns=close_trimmed.columns)\n",
    "offseted_ema200 = pd.DataFrame(columns=close_trimmed.columns)\n",
    "offseted_RSI = pd.DataFrame(columns=close_trimmed.columns)\n",
    "offseted_volume = pd.DataFrame(columns=close_trimmed.columns)\n",
    "for i in np.arange(len(offseted_close.columns)):\n",
    "    offseted_close.iloc[:, i] = np.array(close_trimmed.iloc[start_indexes_for_stocks[i]:-(year_day_size-start_indexes_for_stocks[i]), i])\n",
    "    offseted_ma50.iloc[:, i] = np.array(ma50_trimmed.iloc[start_indexes_for_stocks[i]:-(year_day_size-start_indexes_for_stocks[i]), i])\n",
    "    offseted_ma200.iloc[:, i] = np.array(ma200_trimmed.iloc[start_indexes_for_stocks[i]:-(year_day_size-start_indexes_for_stocks[i]), i])\n",
    "    offseted_ema200.iloc[:, i] = np.array(ema200_trimmed.iloc[start_indexes_for_stocks[i]:-(year_day_size-start_indexes_for_stocks[i]), i])\n",
    "    offseted_RSI.iloc[:, i] = np.array(RSI_trimmed.iloc[start_indexes_for_stocks[i]:-(year_day_size-start_indexes_for_stocks[i]), i])\n",
    "    offseted_volume.iloc[:, i] = np.array(volume_trimmed.iloc[start_indexes_for_stocks[i]:-(year_day_size-start_indexes_for_stocks[i]), i])\n",
    "# we have slightly more than exactly 8 years of clean data, so we trim it some more below"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# change a bit so that info is exactly 8 years, when year = 229 days (data points)\n",
    "# NOTE: if we were addressing testing data, then we would have 2 years of clean data.\n",
    "offseted_volume = offseted_volume.iloc[:1832]  # 458 for test, 1832 for actual data = 229*8\n",
    "offseted_RSI = offseted_RSI.iloc[:1832]  # same for rest\n",
    "offseted_ma200 = offseted_ma200.iloc[:1832]\n",
    "offseted_ma50 = offseted_ma50.iloc[:1832]\n",
    "offseted_ema200 = offseted_ema200.iloc[:1832]\n",
    "offseted_close = offseted_close.iloc[:1832]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the randomly offseted stock and their indicators, For internal use.\n",
    "offseted_close.to_csv('offseted_close.csv', index=False)\n",
    "offseted_ma50.to_csv('offseted_ma50.csv', index=False)\n",
    "offseted_ema200.to_csv('offseted_ema200.csv', index=False)\n",
    "offseted_ma200.to_csv('offseted_ma200.csv', index=False)\n",
    "offseted_RSI.to_csv('offseted_RSI.csv', index=False)\n",
    "offseted_volume.to_csv('offseted_volume.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the offseted data, For internal use.\n",
    "offseted_close = pd.read_csv('offseted_close.csv')\n",
    "offseted_ma50 = pd.read_csv('offseted_ma50.csv')\n",
    "offseted_ma200 = pd.read_csv('offseted_ma200.csv')\n",
    "offseted_ema200 = pd.read_csv('offseted_ema200.csv')\n",
    "offseted_RSI = pd.read_csv('offseted_RSI.csv')\n",
    "offseted_volume = pd.read_csv('offseted_volume.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now that the data is exactly 8 years of 229 days split each indicator into 8 years\n",
    "# We split each indicator so that we have 8 lists, of DataFrames of 1 year of each indicator \n",
    "# for each stock\n",
    "close_dfs = [offseted_close.iloc[i:i+year_day_size] for i in np.arange(0, offseted_close.shape[0], year_day_size)]\n",
    "ma50_dfs = [offseted_ma50.iloc[i:i+year_day_size] for i in np.arange(0, offseted_close.shape[0], year_day_size)]\n",
    "ema200_dfs = [offseted_ema200.iloc[i:i+year_day_size] for i in np.arange(0, offseted_close.shape[0], year_day_size)]\n",
    "ma200_dfs = [offseted_ma200.iloc[i:i+year_day_size] for i in np.arange(0, offseted_close.shape[0], year_day_size)]\n",
    "RSI_dfs = [offseted_RSI.iloc[i:i+year_day_size] for i in np.arange(0, offseted_close.shape[0], year_day_size)]\n",
    "volume_dfs = [offseted_volume.iloc[i:i+year_day_size] for i in np.arange(0, offseted_close.shape[0], year_day_size)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using the above lists, we build the final Data set for the actual learning the testing.\n",
    "# Now the target is did we achieve 10% profit sometime in the next year, with regards to the last\n",
    "# day of the current year. Note that the last year cannot be checked as we dont have the \n",
    "# information of the stock prices for the next year. So, after this process, we are left with 7 \n",
    "# years of learnable, usable, information (1 year for test)\n",
    "# The data set looks as follows: it has a ticked symbol column(each ticker is repeated 7 times),\n",
    "# a close column (raw close price), ma50, ma200, ema200, RSI, volume, and target.\n",
    "# target is set to 1 if we achieved 10% profit as stated above, or 0 otherwise.\n",
    "# Note, that in the columns of the indicators each cell contains a full list of consecutive information, of size 229.\n",
    "complete_df = pd.DataFrame(columns=('ticker', 'close', 'ma50', 'ma200', 'ema200', 'RSI', 'volume', 'target'))\n",
    "years = len(close_dfs) - 1 # this is 7 for data, 1 for test.\n",
    "for i in np.arange(len(offseted_close.columns), dtype=int):  # for each stock\n",
    "    for j in np.arange(years):  # for each year of info except last, because we dont know target\n",
    "        if (close_dfs[j+1].iloc[:, i] / close_dfs[j].iloc[-1, i] > 1.1).any():\n",
    "            target = 1\n",
    "        else: \n",
    "            target= 0\n",
    "        complete_df.at[i*years + j, 'ticker'] = offseted_close.columns[i] \n",
    "        complete_df.at[i*years + j, 'close'] = list(close_dfs[j].iloc[:,i])\n",
    "        complete_df.at[i*years + j, 'ma50'] = list(ma50_dfs[j].iloc[:,i])\n",
    "        complete_df.at[i*years + j, 'ma200'] = list(ma200_dfs[j].iloc[:,i])\n",
    "        complete_df.at[i*years + j, 'ema200'] = list(ema200_dfs[j].iloc[:,i])\n",
    "        complete_df.at[i*years + j, 'RSI'] = list(RSI_dfs[j].iloc[:,i])\n",
    "        complete_df.at[i*years + j, 'volume'] = list(volume_dfs[j].iloc[:,i])\n",
    "        complete_df.at[i*years + j, 'target'] = target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the database, for internal use. We use pickle as it saves without any losses, or problems,\n",
    "# that we otherwise had with csv.\n",
    "# test_set for test, data_set for data\n",
    "complete_df.to_pickle('data_set.pkl')\n",
    "# This concludes the process of raw building of the indicators, and stock information."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-b34017a9",
   "language": "python",
   "display_name": "PyCharm (StockData)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}